from typing import Dict, List, Any
import yaml
from langchain_huggingface import HuggingFaceEndpoint
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage  # Import AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
import os 

class RAGEvaluator:
    """Class to evaluate the quality of RAG responses."""
    
    def __init__(self, config_path: str):
        """Initialize the RAG evaluator with configuration."""
        with open(config_path, 'r') as file:
            self.config = yaml.safe_load(file)
        
        self._initialize_evaluator_llm()
    
    def _initialize_evaluator_llm(self):
        """Initialize the language model for evaluation."""
        llm_provider = self.config['llm']['provider']
        
        if llm_provider == "huggingface":
            # Use HuggingFace for LLM
            self.eval_llm = HuggingFaceEndpoint(
                repo_id=self.config['llm']['model'],
                task="text-generation",
                max_new_tokens=512
            )
        elif llm_provider == "google" :
            # Gemini for evaluation
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-pro",
                temperature=0.7,
                max_tokens=2000,
            )
        else:
            raise ValueError(f"Unsupported LLM provider: {llm_provider}")
        
        #  eval prompt
        self.eval_prompt = ChatPromptTemplate.from_template("""
        You are an expert evaluator for question-answering systems. You will be given:
        1. A question
        2. An answer generated by the system
        3. The context information that was provided to the system
        
        Evaluate the answer based on the following criteria:
        - Relevance: How relevant is the answer to the question? (1-10)
        - Accuracy: How accurate is the answer based on the context? (1-10)
        - Completeness: How complete is the answer? (1-10)
        - Hallucination: Does the answer include information not in the context? (Yes/No)
        
        Question: {question}
        
        Generated Answer: {answer}
        
        Context: {context}
        
        Provide your evaluation scores and a brief explanation for each criterion. Then provide an overall score (1-10).
        """)
    
    def evaluate_response(self, question: str, answer: str, context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Evaluate a question-answer pair against the context.
        
        Args:
            question: The user's question
            answer: The system's answer
            context: The context used to generate the answer
            
        Returns:
            Dictionary with evaluation scores and feedback
        """
        formatted_context = ""
        for doc in context:
            formatted_context += f"Document: {doc['content']}\nSource: {doc['metadata']['source']}\n\n"
        
        evaluation = self.eval_llm.invoke(
            self.eval_prompt.format(
                question=question,
                answer=answer,
                context=formatted_context
            )
        )

        # Check type of 'evaluation' 
        if isinstance(evaluation, AIMessage):
            evaluation_content = evaluation.content
        else:  # its a str
            evaluation_content = evaluation

        
        return {
            "evaluation": evaluation_content,  
            "question": question,
            "answer": answer
        }
    
    def benchmark_system(self, qa_system, test_questions: List[str]) -> Dict[str, Any]:
        """
        Run a benchmark evaluation on a set of test questions.
        
        Args:
            qa_system: The QA system to evaluate
            test_questions: List of questions to test
            
        Returns:
            Dictionary with evaluation results
        """
        results = []
        
        for question in test_questions:
            qa_result = qa_system.answer_question(question)
            
            context_docs = qa_system.retriever.get_relevant_documents(question)
            context = [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in context_docs
            ]
            
            eval_result = self.evaluate_response(question, qa_result["answer"], context)
            results.append(eval_result)
        
        return {
            "evaluated_questions": len(results),
            "results": results
        }